{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "pd.options.display.max_colwidth=100\n",
    "pd.options.display.max_columns=300\n",
    "\n",
    "employees = pd.read_csv(\"../data/employees.csv\")\n",
    "history = pd.read_csv(\"../data/history.csv\")\n",
    "submission = pd.read_csv(\"../data/submission.csv\")\n",
    "\n",
    "# history.loc[:,'Date'] = list(pd.to_datetime(history['Date']))\n",
    "\n",
    "def get_month(text):\n",
    "    if type(text) == str:\n",
    "        numbers = text.split(\"/\")\n",
    "        return int(numbers[0])\n",
    "\n",
    "def get_year(text):\n",
    "    if type(text) == str:\n",
    "        numbers = text.split(\"/\")\n",
    "        return int(numbers[-1])\n",
    "\n",
    "df = history.merge(employees)\n",
    "\n",
    "# for data labeling\n",
    "def label_df(data, n_month=3):\n",
    "    labels = []\n",
    "    for emp in data.EmployeeID.unique():\n",
    "        curr_emp = list(data[data.EmployeeID == emp]['DismissalDate'])\n",
    "        len_emp = len(curr_emp)\n",
    "        if pd.isnull(curr_emp[0]):\n",
    "            labels += [0 for _ in range(len_emp - n_month)] + [2 for _ in range(n_month)]\n",
    "        else:\n",
    "            labels += [0 for _ in range(len_emp - n_month)] + [1 for _ in range(n_month)]\n",
    "    return labels\n",
    "\n",
    "lbls = label_df(df)\n",
    "df['target'] = lbls\n",
    "df = df[df.target!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imblearn\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import random\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def set_seed(random_state=RANDOM_SEED):\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "def model_fit(X_train, y_train):\n",
    "    clf2 = BalancedRandomForestClassifier(n_estimators=200, n_jobs = 8)\n",
    "    clf2 = clf2.fit(X_train, y_train)\n",
    "    return clf2\n",
    "\n",
    "def model_predict(X, models):\n",
    "    preds = models.predict(X)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  1\n",
      "train: 37624\n",
      "val: 4527\n",
      " \n",
      "FOLD #1\n",
      "END OF MODEL FIT\n",
      "Validation Score: 0.2724328109953602\n",
      "fold:  2\n",
      "train: 46644\n",
      "val: 4572\n",
      " \n",
      "FOLD #2\n",
      "END OF MODEL FIT\n",
      "Validation Score: 0.2361719843188047\n",
      "fold:  3\n",
      "train: 55756\n",
      "val: 4719\n",
      " \n",
      "FOLD #3\n",
      "END OF MODEL FIT\n",
      "Validation Score: 0.29528479609929076\n",
      "fold:  4\n",
      "train: 65100\n",
      "val: 4576\n",
      " \n",
      "FOLD #4\n",
      "END OF MODEL FIT\n",
      "Validation Score: 0.30095223691328654\n",
      "MEAN OF SCOREs: 0.27621045708168557\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "columns_to_drop = ['HiringDate', 'DismissalDate']\n",
    "\n",
    "cat_columns = ['DevCenterID', 'SBUID', 'PositionID', 'PositionLevel', \n",
    "               'IsTrainee', 'LanguageLevelID', 'CustomerID', 'ProjectID', \n",
    "               'IsInternalProject', 'OnSite', 'CompetenceGroupID', 'FunctionalOfficeID',\n",
    "               'PaymentTypeId']\n",
    "\n",
    "X = df.drop(columns_to_drop, axis = 1)\n",
    "\n",
    "from category_encoders.basen import BaseNEncoder\n",
    "encoder = BaseNEncoder(cols = cat_columns, base = 2)\n",
    "X = encoder.fit_transform(X)\n",
    "\n",
    "USE_SCALER = True\n",
    "RANDOM_SEED = 1\n",
    "set_seed()\n",
    "\n",
    "X[\"month\"] = X[\"Date\"].apply(get_month)\n",
    "X[\"year\"] = X[\"Date\"].apply(get_year)\n",
    "X = X.sort_values([\"year\",\"month\"])\n",
    "X[\"year_month\"] = X.apply(lambda row: str(row[\"year\"])+\"_\"+str(row[\"month\"]), axis=1)\n",
    "year_month = list(X.year_month.unique())\n",
    "mapping_year_month = dict(zip(year_month, range(len(year_month))))\n",
    "X[\"order_in_time\"] = X[\"year_month\"].map(mapping_year_month)\n",
    "splits = [[[0,9],[10,11]],\n",
    "          [[0,11],[12,13]],\n",
    "          [[0,13],[14,15]],\n",
    "          [[0,15],[16,17]],\n",
    "          [[0,17],[18,19]]]\n",
    "\n",
    "fold = 0\n",
    "scores = []\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=RANDOM_SEED, shuffle=True)\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "for split in splits:\n",
    "    fold += 1\n",
    "    if fold ==5:\n",
    "        continue;\n",
    "    \n",
    "    X_train = X.query(f\"order_in_time >= \"+str(split[0][0])+\" & order_in_time <\"+str(split[0][1]))\n",
    "    X_val = X.query(f\"order_in_time >= \"+str(split[1][0])+\" & order_in_time <\"+str(split[1][1]))\n",
    "    \n",
    "    X_val = X_val.sort_values(by=\"Date\").drop_duplicates(subset=[\"EmployeeID\"], keep=\"last\")\n",
    "    \n",
    "    \n",
    "    y_train = X_train['target']\n",
    "    y_val = list(X_val['target'])\n",
    "    \n",
    "    X_train =X_train.drop(columns=['EmployeeID',\"month\", \"year\", \"year_month\", \"order_in_time\", \"Date\",'target'])\n",
    "    X_val =X_val.drop(columns=['EmployeeID',\"month\", \"year\", \"year_month\", \"order_in_time\", \"Date\",'target'])\n",
    "    \n",
    "    if USE_SCALER:\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "    \n",
    "    print(\"fold: \", fold)\n",
    "    print(\"train:\", len(X_train))\n",
    "    print(\"val:\",len(X_val))\n",
    "    print(\" \")\n",
    "    \n",
    "    print('FOLD #{}'.format(fold))\n",
    "    models = model_fit(X_train, y_train)\n",
    "    print('END OF MODEL FIT')\n",
    "    \n",
    "    y_pred = model_predict(X_val, models)\n",
    "\n",
    "    score = fbeta_score(y_val, y_pred, beta=1.7)\n",
    "    \n",
    "    print('Validation Score: {}'.format(score))\n",
    "    scores.append(score)\n",
    "    \n",
    "mean_score = np.mean(scores)\n",
    "print('MEAN OF SCOREs: {}'.format(mean_score))   \n",
    "\n",
    "\n",
    "y = X.target\n",
    "emps_X = X.EmployeeID\n",
    "X = X.drop(columns=['EmployeeID',\"month\", \"year\", \"year_month\", \"order_in_time\", \"Date\",'target'])\n",
    "\n",
    "model = model_fit(X, y)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "history.loc[:,'Date'] = list(pd.to_datetime(history['Date']))\n",
    "\n",
    "df = history.merge(employees)\n",
    "df['target'] = lbls\n",
    "\n",
    "X_test = df[df.target==2]\n",
    "\n",
    "X_test = X_test[X_test.EmployeeID.isin(set(submission.EmployeeID))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_ids = X_test.EmployeeID\n",
    "X_test = X_test.drop(columns_to_drop, axis = 1)\n",
    "X_test = encoder.transform(X_test)\n",
    "X_test = X_test.drop(columns=['EmployeeID',\"Date\",'target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'EmployeeID':emp_ids, 'target':preds[:,1]})\n",
    "result = pd.DataFrame({'EmployeeID':(result.groupby('EmployeeID').max().target>0.5).index, 'target':(result.groupby('EmployeeID').max().target>0.5).astype(int).values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('Mykola_initial_submit_max_score.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4156"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv(\"../data/submission.csv\")\n",
    "len(submission.EmployeeID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EmployeeID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00116D71-E87D-4B64-A566-1F29B2A798A8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0034ECA4-0562-4AC7-A826-4AE81C64D69F</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00384806-F711-41BA-A924-8F27E996F891</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>005B5FD6-FD19-4924-98E4-4C06F7F6BF2C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0061CAE7-B123-46B0-9BF7-E1E94E9AD80B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4151</th>\n",
       "      <td>FFCFA379-9529-49EF-87BA-1522FE94B415</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4152</th>\n",
       "      <td>FFE9E1F0-1DB1-4BA8-A8FB-026E7DBCF49F</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4153</th>\n",
       "      <td>FFEBB9DA-B0CF-49AE-91D3-14A0BF22219E</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4154</th>\n",
       "      <td>FFED12A3-5B28-4101-908A-2851CBADE045</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4155</th>\n",
       "      <td>FFF3B179-1D20-40FF-A330-A051BDF37301</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4156 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                EmployeeID  target\n",
       "0     00116D71-E87D-4B64-A566-1F29B2A798A8       0\n",
       "1     0034ECA4-0562-4AC7-A826-4AE81C64D69F       0\n",
       "2     00384806-F711-41BA-A924-8F27E996F891       1\n",
       "3     005B5FD6-FD19-4924-98E4-4C06F7F6BF2C       1\n",
       "4     0061CAE7-B123-46B0-9BF7-E1E94E9AD80B       0\n",
       "...                                    ...     ...\n",
       "4151  FFCFA379-9529-49EF-87BA-1522FE94B415       0\n",
       "4152  FFE9E1F0-1DB1-4BA8-A8FB-026E7DBCF49F       1\n",
       "4153  FFEBB9DA-B0CF-49AE-91D3-14A0BF22219E       1\n",
       "4154  FFED12A3-5B28-4101-908A-2851CBADE045       0\n",
       "4155  FFF3B179-1D20-40FF-A330-A051BDF37301       1\n",
       "\n",
       "[4156 rows x 2 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building train and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building train and test datasets:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "pd.options.display.max_colwidth=100\n",
    "pd.options.display.max_columns=300\n",
    "\n",
    "employees = pd.read_csv(\"../data/employees.csv\")\n",
    "history = pd.read_csv(\"../data/history.csv\")\n",
    "submission = pd.read_csv(\"../data/submission.csv\")\n",
    "\n",
    "history.loc[:,'Date'] = list(pd.to_datetime(history['Date']))\n",
    "\n",
    "def get_month(text):\n",
    "    if type(text) == str:\n",
    "        numbers = text.split(\"/\")\n",
    "        return int(numbers[0])\n",
    "\n",
    "def get_year(text):\n",
    "    if type(text) == str:\n",
    "        numbers = text.split(\"/\")\n",
    "        return int(numbers[-1])\n",
    "\n",
    "df = history.merge(employees)\n",
    "\n",
    "# for data labeling\n",
    "def label_df(data, n_month=3):\n",
    "    labels = []\n",
    "    for emp in data.EmployeeID.unique():\n",
    "        curr_emp = list(data[data.EmployeeID == emp]['DismissalDate'])\n",
    "        len_emp = len(curr_emp)\n",
    "        if pd.isnull(curr_emp[0]):\n",
    "            labels += [0 for _ in range(len_emp - n_month+2)] + [2 for _ in range(n_month-2)]\n",
    "        else:\n",
    "            labels += [0 for _ in range(len_emp - n_month)] + [1 for _ in range(n_month)]\n",
    "    return labels\n",
    "\n",
    "lbls = label_df(df)\n",
    "df['target'] = lbls\n",
    "df = df[df.target!=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "columns_to_drop = ['Date','HiringDate', 'DismissalDate']\n",
    "\n",
    "cat_columns = ['DevCenterID', 'SBUID', 'PositionID', 'PositionLevel', \n",
    "               'IsTrainee', 'LanguageLevelID', 'CustomerID', 'ProjectID', \n",
    "               'IsInternalProject', 'OnSite', 'CompetenceGroupID', 'FunctionalOfficeID',\n",
    "               'PaymentTypeId']\n",
    "\n",
    "X = df.drop(columns_to_drop, axis = 1)\n",
    "\n",
    "from category_encoders.basen import BaseNEncoder\n",
    "encoder = BaseNEncoder(cols = cat_columns, base = 2)\n",
    "\n",
    "X = encoder.fit_transform(X)\n",
    "X = X.reset_index()\n",
    "\n",
    "tmp = X.EmployeeID\n",
    "X.drop('EmployeeID', axis = 1, inplace = True)\n",
    "cols = X.columns\n",
    "\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns = cols)\n",
    "X['EmployeeID'] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = X[X.EmployeeID.isin(set(submission.EmployeeID))]\n",
    "test_set = test_set.groupby('EmployeeID').tail(5)\n",
    "train_set = X.drop(test_set.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test dataset:\n",
    "import tensorflow as tf\n",
    "\n",
    "cols = list(X.columns)\n",
    "cols.pop(-1)\n",
    "cols.pop(-1)\n",
    "\n",
    "X_test = [person[1][cols].values for person in test_set.groupby('EmployeeID')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5216/5216 [00:00<00:00, 8106.50it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37498\n",
      "37498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### train dataset:\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "cols = list(X.columns)\n",
    "cols.pop(-1)\n",
    "\n",
    "data = [person[1][cols].values for person in train_set.groupby('EmployeeID')]\n",
    "\n",
    "# getting subsequences:\n",
    "def get_subsequence(sequence):\n",
    "    if sequence.shape[0]<6:\n",
    "        return []\n",
    "#     if (sequence.shape[0]>=2) and (sequence.shape[0]<6):\n",
    "#         return [sequence]\n",
    "    else:\n",
    "        tmp_data = []\n",
    "        for i in range(0, sequence.shape[0]-5):\n",
    "            tmp_data.append(sequence[i:i+6])\n",
    "        return tmp_data\n",
    "\n",
    "sub_data = []\n",
    "for x in tqdm(data):\n",
    "    sub_data = sub_data+ get_subsequence(x)\n",
    "\n",
    "x_train = [x[:-1,:-1] for x in sub_data]\n",
    "\n",
    "# y_train = tf.keras.utils.to_categorical([x[-1, -1] for x in data])   # Expects class labels from 0 to n (-> subtract 1).\n",
    "y_train = [x[-1, -1] for x in sub_data]\n",
    "print(len(x_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      " 9038/31873 [=======>......................] - ETA: 2:13 - loss: 0.2977 - acc: 0.9251 - f1_m: 4.4258e-04 - precision_m: 4.4258e-04 - recall_m: 4.4258e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-649-73eeafc121ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1426\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(5, input_shape=(None, 91), dropout=0.5, recurrent_dropout=0.5))  # LSTM for arbitrary length series.\n",
    "model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc',f1_m,precision_m, recall_m])\n",
    "\n",
    "class TrainGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Need to expand arrays to have batch size 1.\n",
    "        return self.x[index][None, :, :], self.y[index][None]\n",
    "\n",
    "model.fit_generator(TrainGenerator(X_train, y_train), epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(TrainGenerator(X_train, y_train), epochs=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [model.predict(x[None, :, :]).ravel() for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([x[0] for x in pred])\n",
    "pred = np.array(pred)\n",
    "# pred>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# history.loc[:,'Date'] = list(pd.to_datetime(history['Date']))\n",
    "\n",
    "# df = history.merge(employees)\n",
    "# df['target'] = lbls\n",
    "\n",
    "# X_test = df[df.target==2]\n",
    "# X_test = X_test[X_test.Date == datetime(2019,2,1)]\n",
    "\n",
    "# X_test = X_test[X_test.EmployeeID.isin(set(submission.EmployeeID))]\n",
    "# emp_ids = X_test.EmployeeID\n",
    "\n",
    "result = pd.DataFrame({'EmployeeID':emp_ids, 'target':(pred>0.03).astype(int)})\n",
    "\n",
    "result.to_csv('Mykola_initial_LSTM_submit_super_dummie_3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
